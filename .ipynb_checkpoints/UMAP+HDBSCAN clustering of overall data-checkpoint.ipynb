{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "sys.path.insert(0, '../../')\n",
    "sys.path.insert(0, '../../cycif/')\n",
    "from get_data import file2frame\n",
    "from cycif import *\n",
    "from common_apis import *\n",
    "import random\n",
    "random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP dimension reduction need be run only once\n",
    "os.chdir('D:/data')\n",
    "umap_dist = 'correlation'\n",
    "umap_nn = 5\n",
    "umap_md = 0.1\n",
    "umap_data_fn = ' '.join(['MCF10A commons 533k cells UMAP data v5', 'distfun_', umap_dist, 'NN_', str(umap_nn),'minDist_', str(umap_md),'.csv'])\n",
    "df_pooled_time_umap = pd.read_csv(umap_data_fn,index_col=0)\n",
    "df_pooled = pd.read_csv('MCF10A 533k quantile normed data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HDBSCAN(algorithm='best', allow_single_cluster=False, alpha=1.0,\n",
       "    approx_min_span_tree=True, cluster_selection_method='eom',\n",
       "    core_dist_n_jobs=4, gen_min_span_tree=False, leaf_size=40,\n",
       "    match_reference_implementation=False, memory='d:/temp',\n",
       "    metric='euclidean', min_cluster_size=491, min_samples=15, p=None,\n",
       "    prediction_data=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering = HDBSCAN(min_cluster_size = 491,min_samples=15,memory='d:/temp')\n",
    "clustering.fit(df_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two step HDBSCAN clustering to generate cluster information on the dataset\n",
    "# Firt step: overal clustering\n",
    "# Second step: break the largest cluster, which is often the DMSO cluter, into small clusters.\n",
    "clustering = HDBSCAN(min_cluster_size = 491,min_samples=15,memory='d:/temp')\n",
    "clustering.fit(df_pooled_time_umap.iloc[:,:2])\n",
    "cluster_counts = np.unique(clustering.labels_, return_counts=True)[1]\n",
    "DMSO_cluster_id = cluster_counts.argmax()-1\n",
    "idx_DMSO_cluster = [i for i, x in enumerate(clustering.labels_) if x == DMSO_cluster_id]\n",
    "print(np.unique(clustering.labels_, return_counts=True))\n",
    "subclustering = HDBSCAN(min_cluster_size = 500)\n",
    "subclustering.fit(df_pooled_time_umap.iloc[idx_DMSO_cluster,:2])\n",
    "print(np.unique(subclustering.labels_, return_counts=True))\n",
    "df_labels = pd.Series(['Overall ' + str(x) for x in clustering.labels_])\n",
    "df_labels[idx_DMSO_cluster] = ['DMSO_' + str(x) for x in subclustering.labels_]\n",
    "df_pooled_time_umap['cluster'] = df_labels.values\n",
    "df_pooled_time_umap.to_csv(umap_data_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making cluster plot of first iteration\n",
    "plt.figure(figsize=(32,18))\n",
    "labels = pd.Series(clustering.labels_)\n",
    "df_low_dim = df_pooled_time_umap.iloc[:,:2].values\n",
    "for cluster in np.unique(labels)[1:]:\n",
    "    cells_idx = labels[labels==cluster].index.values\n",
    "    print('Processing cluster: {} with {} cells'.format(str(cluster),str(len(cells_idx))))\n",
    "    plt.scatter(df_low_dim[cells_idx,0],df_low_dim[cells_idx,1],label=cluster, s = 0.25)\n",
    "plt.legend(markerscale = 50,prop={'size': 32}, bbox_to_anchor = (1.1,1))\n",
    "plt.savefig('Overall Umap scatterplot clustered one iteration.png')\n",
    "plt.close()\n",
    "\n",
    "# Make subclustering on the DMSO central cluster\n",
    "plt.figure(figsize=(32,18))\n",
    "df_low_dim = df_pooled_time_umap.iloc[:,:2].values\n",
    "for cluster in sorted(np.unique(df_labels)[1:]):\n",
    "    cells_idx = df_labels[df_labels==cluster].index.values\n",
    "#     print('Processing cluster: {} with {} cells'.format(str(cluster),str(len(cells_idx))))\n",
    "    plt.scatter(df_low_dim[cells_idx,0],df_low_dim[cells_idx,1],label=cluster, s = 0.25)\n",
    "plt.legend(markerscale = 50,prop={'size': 24}, ncol=8, bbox_to_anchor = (1.02,0))\n",
    "plt.savefig('Overall Umap scatterplot clutered two iterations.png',bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "\n",
    "df_pooled_time_umap['condition'] = df_pooled_time_umap.Drug + '_' + df_pooled_time_umap.Conc.astype(str) + '_' + df_pooled_time_umap.time\n",
    "df_freq = df_pooled_time_umap.groupby(['cluster','condition']).count().iloc[:,0]\n",
    "cluster_siezs = df_freq.groupby('cluster').sum()\n",
    "condition_sizes = df_pooled_time_umap.groupby(['condition']).count().iloc[:,0]\n",
    "\n",
    "df_cluster_distribution_pval = pd.DataFrame(index = cluster_siezs.index, columns=condition_sizes.index)\n",
    "for cluster in cluster_siezs.index:\n",
    "    N = cluster_siezs[cluster]\n",
    "    k_vector = df_freq[cluster]-1\n",
    "    n_vector = condition_sizes.loc[k_vector.index]\n",
    "    pval_vector = hypergeom.sf(k_vector,len(df_pooled_time_umap),n_vector,N)\n",
    "    log_pval_vector = -np.log10(pval_vector)\n",
    "    # suppress super low pvalues\n",
    "    log_pval_vector[log_pval_vector>=400] = 318\n",
    "    df_cluster_distribution_pval.loc[cluster, k_vector.index] = log_pval_vector\n",
    "\n",
    "df_cluster_distribution_pval.fillna(0,inplace=True)    \n",
    "df_cluster_distribution_pval = abs(df_cluster_distribution_pval).transpose()\n",
    "df_cluster_distribution_pval = np.log10(df_cluster_distribution_pval+1)\n",
    "# sinificant threshold at 0.05 is now 0.36\n",
    "for i,col in enumerate(['Time','Dose','Drug']):\n",
    "    df_cluster_distribution_pval.insert(0,col,[x.split('_')[2-i] for x in df_cluster_distribution_pval.index])\n",
    "df_cluster_distribution_pval.to_csv('Cluster distribution hypergeom pval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for best parameters in UMAP optional\n",
    "# Define sub_df from all the data first\n",
    "random_idx = np.random.choice(df_pooled.index,10000, False)\n",
    "sub_df = df_pooled.loc[random_idx].iloc[:,:2]\n",
    "list_nn = [5,15,25,35,45,55,65,75,85]\n",
    "lsit_md = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "_,axes = plt.subplots(ncols=3, nrows=3,figsize = (32,18))\n",
    "axes = axes.ravel()\n",
    "idx = 0\n",
    "for n_neighbors in list_nn:\n",
    "    umap = UMAP(n_neighbors = n_neighbors,n_components=2,min_dist=0.1, metric='correlation', random_state=50)\n",
    "    df_pooled_time_umap = umap.fit_transform(sub_df)\n",
    "    metadata = pd.read_csv('MCF10A commons metadata.csv',index_col=0)\n",
    "    df_pooled_time_umap = pd.DataFrame(df_pooled_time_umap,columns = ['X' + str(i) for i in range(df_pooled_time_umap.shape[1])])\n",
    "    axes[idx].scatter(df_pooled_time_umap.X0, df_pooled_time_umap.X1, s=0.01)\n",
    "    axes[idx].set_title('n_neighbors: {}'.format(str(n_neighbors)))\n",
    "    idx+=1\n",
    "\n",
    "_,axes = plt.subplots(ncols=3, nrows=3,figsize = (32,18))\n",
    "axes = axes.ravel()\n",
    "idx = 0    \n",
    "for md in lsit_md:\n",
    "    umap = UMAP(n_neighbors = 5,n_components=2,min_dist=md, metric='correlation',random_state=50)\n",
    "    df_pooled_time_umap = umap.fit_transform(sub_df)\n",
    "    metadata = pd.read_csv('MCF10A commons metadata.csv',index_col=0)\n",
    "    df_pooled_time_umap = pd.DataFrame(df_pooled_time_umap,columns = ['X' + str(i) for i in range(df_pooled_time_umap.shape[1])])\n",
    "    axes[idx].scatter(df_pooled_time_umap.X0, df_pooled_time_umap.X1, s=0.01)\n",
    "    axes[idx].set_title('min_dist: {}'.format(str(md)))\n",
    "    idx+=1\n",
    "\n",
    "# # HDBSCAN peremeter optimization, optional\n",
    "# cp_list = []\n",
    "# mcs_range = range(10,2000)\n",
    "# for mcs in mcs_range:\n",
    "#     clustering = HDBSCAN(min_cluster_size = mcs,min_samples=15,memory='d:/temp')\n",
    "#     clustering.fit(df_pooled_time_umap.iloc[:,:2])\n",
    "#     cp = np.median(clustering.cluster_persistence_)\n",
    "#     cp_list.append(cp)\n",
    "    \n",
    "# sns.lineplot(mcs_range,cp_list)\n",
    "# min_cluster_size = cp_list.index(max(cp_list))\n",
    "# print('minimal cluster size based on maximal cluster persistence: {}'.format(str(min_cluster_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
